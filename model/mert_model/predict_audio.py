import os
import torch
import torch.nn as nn
import librosa  # <--- –ò—Å–ø–æ–ª—å–∑—É–µ–º Librosa
import pandas as pd
import numpy as np
from transformers import Wav2Vec2FeatureExtractor, AutoModel

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
CSV_FILE = '../../datasets/MTG/moodtheme_mp3.csv'
MODEL_PATH = "trained_model/mert_model_best.pth"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MERT_HF_ID = "m-a-p/MERT-v1-95M"


# --- 1. –ö–õ–ê–°–° –í–ê–®–ï–ô –ú–û–î–ï–õ–ò ---
class MERTClassifier(nn.Module):
    def __init__(self, input_size=768, num_classes=56):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.network(x)


# --- 2. –ö–õ–ê–°–° –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø ---
class AudioPredictor:
    def __init__(self):
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ MERT ({MERT_HF_ID})...")
        self.processor = Wav2Vec2FeatureExtractor.from_pretrained(MERT_HF_ID, trust_remote_code=True)
        self.mert_model = AutoModel.from_pretrained(MERT_HF_ID, trust_remote_code=True).to(DEVICE)
        self.mert_model.eval()

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        if os.path.exists(CSV_FILE):
            df = pd.read_csv(CSV_FILE, nrows=1)
            meta_cols = ['TRACK_ID', 'PATH', 'DURATION']
            self.labels = [col for col in df.columns if col not in meta_cols]
        else:
            self.labels = []  # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–æ –±–µ–∑ –∏–º–µ–Ω —Ç–µ–≥–æ–≤

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–∞—à—É –º–æ–¥–µ–ª—å
        self.classifier = MERTClassifier(num_classes=len(self.labels)).to(DEVICE)
        try:
            self.classifier.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
            self.classifier.eval()
            print("–í–∞—à–∞ –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ.")
        except Exception as e:
            print(f"–û–®–ò–ë–ö–ê –≤–µ—Å–æ–≤: {e}")

    def process_audio(self, audio_path):
        """–í–µ—Ä—Å–∏—è —Å Librosa"""
        try:
            # 1. –ó–∞–≥—Ä—É–∑–∫–∞ + –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ + –ú–æ–Ω–æ (–≤—Å–µ –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ—á–∫–µ!)
            # sr=24000 ‚Äî —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ MERT
            # mono=True ‚Äî —Å–º–µ—à–∏–≤–∞–µ–º –∫–∞–Ω–∞–ª—ã
            audio_array, _ = librosa.load(audio_path, sr=24000, mono=True)

            # 2. –û–±—Ä–µ–∑–∫–∞ (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 30 —Å–µ–∫, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–∏—Ç—å –ø–∞–º—è—Ç—å GPU)
            max_samples = 24000 * 30
            if len(audio_array) > max_samples:
                audio_array = audio_array[:max_samples]

            # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
            # Librosa –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç numpy, processor –µ–≥–æ –æ—Ç–ª–∏—á–Ω–æ –ø–æ–Ω–∏–º–∞–µ—Ç
            input_values = self.processor(audio_array,
                                          sampling_rate=24000,
                                          return_tensors="pt").input_values.to(DEVICE)

            # 4. –ü—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ MERT
            with torch.no_grad():
                outputs = self.mert_model(input_values)
                # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                embedding = outputs.last_hidden_state.mean(dim=1)

            return embedding

        except Exception as e:
            print(f"Librosa error: {e}")
            return None

    def predict(self, audio_path):
        if not os.path.exists(audio_path):
            print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        print(f"Processing: {os.path.basename(audio_path)} ...")
        embedding = self.process_audio(audio_path)

        if embedding is None: return

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏
        with torch.no_grad():
            logits = self.classifier(embedding)
            probs = torch.sigmoid(logits)[0].cpu().numpy()

        # –í—ã–≤–æ–¥
        top_indices = probs.argsort()[::-1]
        print(f"\nüéß –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        for i in range(5):
            idx = top_indices[i]
            if probs[idx] > 0.05:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Ö–æ—Ç—å –Ω–µ–º–Ω–æ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ
                print(f"{probs[idx]:.1%} | {self.labels[idx]}")
        print("-" * 30)


if __name__ == "__main__":
    predictor = AudioPredictor()

    while True:
        path = input("\n>> –ü—É—Ç—å –∫ mp3/wav (–∏–ª–∏ 'exit'): ").strip().strip('"')
        if path.lower() in ['exit', 'quit']: break
        predictor.predict(path)